{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGdy4uJq1kSQ"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HNJ04EH1qdW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output \n",
        "!pip install gcsfs\n",
        "!pip install t5==0.9.2\n",
        "!pip install -q tensorflow-text==2.8.0rc0\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g65Qu6MF1x2W"
      },
      "outputs": [],
      "source": [
        "print(\"Installing dependencies...\")\n",
        "import functools\n",
        "import os\n",
        "import gin\n",
        "import tensorflow_gcs_config\n",
        "from google.colab import auth\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "import t5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2h1MRzBLtex2"
      },
      "outputs": [],
      "source": [
        "print(\"Setting up GCS access...\")\n",
        "os.environ['USE_AUTH_EPHEM'] = '0'\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set credentials for GCS reading/writing from Colab and TPU.\n",
        "TPU_TOPOLOGY = \"2x2\"\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  TPU_ADDRESS = tpu.get_master()\n",
        "  print('Running on TPU:', TPU_ADDRESS)\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "\n",
        "#LOGGING\n",
        "tf.get_logger().propagate = False\n",
        "py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0kxnD0T8EzI"
      },
      "source": [
        "# Load Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5h4PkSo_Kus"
      },
      "outputs": [],
      "source": [
        "TOKENIZER_DIR = \"gs://cs-summarizer/linking&classification\" #@param { type: \"string\" }\n",
        "if not TOKENIZER_DIR or TOKENIZER_DIR == \"gs://\": \n",
        "  raise ValueError(\"You must enter a TOKENIZER_DIR.\")\n",
        "\n",
        "VOCAB_PREFIX = 'tokenizer/tokenizer-linking' #@param {type: \"string\"}\n",
        "vocab_model_path = os.path.join(TOKENIZER_DIR, f'{VOCAB_PREFIX}.model')\n",
        "vocab_path = os.path.join(TOKENIZER_DIR, f'{VOCAB_PREFIX}.vocab')\n",
        "print(vocab_model_path)\n",
        "print(vocab_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wq4YkN9_UkO"
      },
      "outputs": [],
      "source": [
        "from t5.data import postprocessors as t5_postprocessors\n",
        "from t5.seqio import Feature,SentencePieceVocabulary\n",
        "\n",
        "num_special_mask_tokens = 100 #@param {type: \"integer\"}\n",
        "\n",
        "def load_vocabulary():\n",
        "  return SentencePieceVocabulary(vocab_model_path, num_special_mask_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3yMW-___hYd"
      },
      "source": [
        "# Prepare Dataset for T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glLJUm1dxIiH"
      },
      "outputs": [],
      "source": [
        "train_path_linking = 'gs://cs-summarizer/linking&classification/datasets/linking/train.tsv' #@param { type: \"string\" }\n",
        "eval_path_linking = 'gs://cs-summarizer/linking&classification/datasets/linking/eval.tsv' #@param { type: \"string\" }\n",
        "test_path_linking = 'gs://cs-summarizer/linking&classification/datasets/linking/test.tsv' #@param { type: \"string\" }\n",
        "finetune_datasets_paths_linking = {\n",
        "    \"train\":      train_path_linking,\n",
        "    \"validation\": test_path_linking\n",
        "}\n",
        "\n",
        "train_path_classification = 'gs://cs-summarizer/linking&classification/datasets/classification/train.tsv' #@param { type: \"string\" }\n",
        "eval_path_classification = 'gs://cs-summarizer/linking&classification/datasets/classification/eval.tsv' #@param { type: \"string\" }\n",
        "test_path_classification = 'gs://cs-summarizer/linking&classification/datasets/classification/test.tsv' #@param { type: \"string\" }\n",
        "finetune_datasets_paths_classification = {\n",
        "    \"train\":      train_path_classification,\n",
        "    \"validation\": test_path_classification\n",
        "}\n",
        "\n",
        "\n",
        "num_input_examples_linking = dict(train=3992, validation=704)  \n",
        "num_input_examples_classification = dict(train=9250, validation=1157) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0NTLbyXvkCs"
      },
      "outputs": [],
      "source": [
        "def load_dataset_linking(split, shuffle_files=True):\n",
        "  \"\"\"\n",
        "  Function to load .tsv dataset as a tf.data.Dataset in TensorFlow\n",
        "  \"\"\"\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "\n",
        "  ds = tf.data.TextLineDataset(finetune_datasets_paths_linking[split])\n",
        "  ds = ds.map(functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=True)\n",
        "                          , \n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "def load_dataset_classification(split, shuffle_files=True):\n",
        "  \"\"\"\n",
        "  Function to load .tsv dataset as a tf.data.Dataset in TensorFlow\n",
        "  \"\"\"\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "\n",
        "  ds = tf.data.TextLineDataset(finetune_datasets_paths_classification[split])\n",
        "  ds = ds.map(functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=True)\n",
        "                          , \n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J4_UFsVmSPk"
      },
      "source": [
        "### A few examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__RNWuimAxS9"
      },
      "outputs": [],
      "source": [
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(load_dataset_classification(\"validation\").take(5)):\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsbHi89ZA5-j"
      },
      "source": [
        "# Dataset Prepocessing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bJZPQgjxKZ1"
      },
      "outputs": [],
      "source": [
        "from tensorflow_datasets.core.utils.type_utils import Shape\n",
        "def preprocessing_linking(ds):\n",
        "  \"\"\"\n",
        "  Preprocess function to convert the tf.data.Dataset into a text-to-text format,\n",
        "  with both inputs and targets fields.\n",
        "  Param: tf.data.Dataset\n",
        "  Return: text-to-text format\n",
        "  \"\"\"\n",
        "  prefix = 'LINK CODE TO COMMENT: ' #@param {type : \"string\"}\n",
        "  def to_inputs_and_targets(ex):\n",
        "    x_input = tf.strings.strip(prefix + ex['input'])\n",
        "    y_label = tf.strings.strip(ex['output']) \n",
        "    inputs = tf.strings.join([x_input], separator=' ')\n",
        "    class_label = tf.strings.join([y_label], separator=' ')\n",
        "    return {'inputs': inputs, 'targets': class_label}\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  from tensorflow_datasets.core.utils.type_utils import Shape\n",
        "\n",
        "def preprocessing_classification(ds):\n",
        "  \"\"\"\n",
        "  Preprocess function to convert the tf.data.Dataset into a text-to-text format,\n",
        "  with both inputs and targets fields.\n",
        "  Param: tf.data.Dataset\n",
        "  Return: text-to-text format\n",
        "  \"\"\"\n",
        "  prefix = 'CODE COMMENT CLASSIFICATION: ' #@param {type : \"string\"}\n",
        "  def to_inputs_and_targets(ex):\n",
        "    x_input = tf.strings.strip(prefix + ex['input'])\n",
        "    y_label = tf.strings.strip(ex['output']) \n",
        "    inputs = tf.strings.join([x_input], separator=' ')\n",
        "    class_label = tf.strings.join([y_label], separator=' ')\n",
        "    return {'inputs': inputs, 'targets': class_label}\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vymEYLbQCBRY"
      },
      "source": [
        "### A few examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOyas1ZqBVgE"
      },
      "outputs": [],
      "source": [
        "print(\"A few preprocessed train examples...\")\n",
        "sample = tfds.as_numpy(preprocessing_linking(load_dataset_linking(\"train\").take(5)))\n",
        "for ex in sample:\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OkQeLeh8Rst"
      },
      "source": [
        "# Creating Task and Mixture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PobLvzL18zzR"
      },
      "outputs": [],
      "source": [
        "DEFAULT_OUTPUT_FEATURES = {\n",
        "    \"inputs\": Feature(\n",
        "        vocabulary=load_vocabulary(), add_eos=True, required=False),\n",
        "    \"targets\": Feature(\n",
        "        vocabulary=load_vocabulary(), add_eos=True)\n",
        "    }\n",
        "\n",
        "TASK_NAME_LINKING = \"linking\" #@param{ type : \"string\"}\n",
        "\n",
        "# TASK\n",
        "t5.data.TaskRegistry.remove(TASK_NAME_LINKING)\n",
        "t5.data.TaskRegistry.add(\n",
        "    TASK_NAME_LINKING,\n",
        "    # Function which returns a tf.data.Dataset\n",
        "    dataset_fn=load_dataset_linking,\n",
        "    splits=[\"train\",\"validation\"],\n",
        "    # List of functions that preprocess the input tf.data.Dataset\n",
        "    text_preprocessor=[preprocessing_linking],\n",
        "    # Accuracy is used as evaluation metric\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    # Not required, helps for mixing and auto-caching\n",
        "    # num_input_examples=num_input_examples,\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES\n",
        ")\n",
        "\n",
        "TASK_NAME_CLASSIFICATION = \"classification\" #@param{ type : \"string\"}\n",
        "\n",
        "t5.data.TaskRegistry.remove(TASK_NAME_CLASSIFICATION)\n",
        "t5.data.TaskRegistry.add(\n",
        "    TASK_NAME_CLASSIFICATION,\n",
        "    # Function which returns a tf.data.Dataset\n",
        "    dataset_fn=load_dataset_classification,\n",
        "    splits=[\"train\",\"validation\"],\n",
        "    # List of functions that preprocess the input tf.data.Dataset\n",
        "    text_preprocessor=[preprocessing_classification],\n",
        "    # Accuracy is used as evaluation metric\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    # Not required, helps for mixing and auto-caching\n",
        "    # num_input_examples=num_input_examples,\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES\n",
        ")\n",
        "\n",
        "MIXTURE_NAME = \"task\" #@param{ type : \"string\"}\n",
        "\n",
        "# MIXTURE\n",
        "t5.data.MixtureRegistry.remove(MIXTURE_NAME)\n",
        "t5.data.MixtureRegistry.add(\n",
        "    MIXTURE_NAME,\n",
        "    # List of tasks\n",
        "    [TASK_NAME_LINKING, TASK_NAME_CLASSIFICATION],\n",
        "    default_rate=1.0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwqLrVdGB6yy"
      },
      "source": [
        "### A few examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVHcIrjkA93r"
      },
      "outputs": [],
      "source": [
        "finetuning_task = t5.data.TaskRegistry.get(TASK_NAME_CLASSIFICATION)\n",
        "ds = finetuning_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 1250, \"targets\": 256})\n",
        "print(\"A few preprocessed training examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQobwjpVCJbu"
      },
      "source": [
        "# Creating Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wFs8zZ29Htp"
      },
      "outputs": [],
      "source": [
        "scheduler = \"constant\" #@param [\"polynomial\", \"constant\", \"isr\", \"slanted\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKKGuWpXCMNV",
        "outputId": "c432bbef-a796-49d9-cfca-2336269f1bad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<absl.flags._flagvalues.FlagHolder at 0x7f2af7fa2410>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Storage paths\n",
        "FINETUNE_MODEL_DIR =  \"gs://cs-summarizer/linking&classification/models/with-pretraining-line-level/Best-Performing-Model\"\n",
        "PRETRAIN_MODEL_DIR='gs://cs-summarizer/linking&classification/models/pretrained-model'\n",
        "FLAGS = tf.app.flags.FLAGS\n",
        "tf.app.flags.DEFINE_string ('f', '', 'kernel')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmaDaY39DwxR"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "\n",
        "# Learning rate properties\n",
        "starter_learning_rate = 0.01 #@param {type : \"number\"}\n",
        "end_learning_rate = 0.001 #@param {type : \"number\"}\n",
        "decay_steps = 10000 #@param {type : \"integer\"}\n",
        "\n",
        "learning_rate_fn = PolynomialDecay(\n",
        "     starter_learning_rate,\n",
        "     decay_steps,\n",
        "     end_learning_rate,\n",
        "     power=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3Qx699vN302"
      },
      "outputs": [],
      "source": [
        "from mesh_tensorflow.transformer.learning_rate_schedules import slanted_triangular, truncated_rsqrt\n",
        "from t5 import models\n",
        "\n",
        "# Learning rate schedule fn\n",
        "if scheduler == 'polynomial':\n",
        "  learning_rate_scheduler = learning_rate_fn\n",
        "elif scheduler == 'isr':\n",
        "  learning_rate_scheduler = truncated_rsqrt\n",
        "elif scheduler == 'slanted':\n",
        "  learning_rate_scheduler = slanted_triangular\n",
        "else: \n",
        "  learning_rate_scheduler = 0.001\n",
        "\n",
        "print(learning_rate_scheduler)\n",
        "\n",
        "# Model properties\n",
        "MODEL_SIZE = \"small\" \n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 32, 100),\n",
        "    \"base\": (2, 128, 8),\n",
        "    \"large\": (8, 64, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
        "\n",
        "\n",
        "# Mesh Tensorflow Transformer\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=FINETUNE_MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    tpu_topology=TPU_TOPOLOGY,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size, \n",
        "    sequence_length={\"inputs\": 1250, \"targets\": 256}, \n",
        "    # pick the correct scheduler, according to the model you want to train\n",
        "    learning_rate_schedule = learning_rate_scheduler, \n",
        "    save_checkpoints_steps=5000,\n",
        "    keep_checkpoint_max=keep_checkpoint_max,\n",
        "    iterations_per_loop=100,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR7ZLccg9cwS"
      },
      "source": [
        "# Learning Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5puQKZ4EuQ1"
      },
      "outputs": [],
      "source": [
        "#Upload the operative_config.gin according to the scheduler that has been picked\n",
        "LOCAL_GIN_PATH = \"/content/operative_config.gin\"\n",
        "with gin.unlock_config():\n",
        "    gin.parse_config_file(LOCAL_GIN_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4tR2WUl-T96"
      },
      "source": [
        "# Finetuning the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oHp5ScE7nf2"
      },
      "outputs": [],
      "source": [
        "TRAIN_STEPS =  75000#@param {type: \"integer\"}\n",
        "\n",
        "# Start finetuning\n",
        "#1) If no pre-training, then method model.train\n",
        "\n",
        "model.finetune(mixture_or_task_name=MIXTURE_NAME,\n",
        "               finetune_steps=TRAIN_STEPS, \n",
        "               pretrained_model_dir=PRETRAIN_MODEL_DIR)\n",
        "\n",
        "# model.train(mixture_or_task_name=MIXTURE_NAME, steps=TRAIN_STEPS)\n",
        "\n",
        "model.eval(\n",
        "    mixture_or_task_name=MIXTURE_NAME,\n",
        "    checkpoint_steps=-1\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Multi_Task_Linking&Classification.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}