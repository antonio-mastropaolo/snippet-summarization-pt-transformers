# baseline

We created the baseline for our model from the paper `Automatically detecting the scopes of source code comments`

There are three different features: code features, comment features and code/comment features.

## STEP 0: before starting
We set some global variable at the beginning of the main file that must be defined before running the code.

##### dataset (path to train, eval, and test set files) We will save here some information about comments
input_folder=<path_to_the_dataset_folder>
##### xml dataset (we will save here the csv files containing the xml generated by SrcML
The data can be found in `xml` folder
xml_folder=<path_to_the_dataset_folder>
##### output folder where we save the skipgram model and the feature vectors
output_folder=<path_to_the_dataset_folder>

## STEP 1: train the skipgram model
The authors trained a skipgram model on a corpus of java methods and comments, in order to align the natural language and the code (i.e., NL referred to a specific snippet should be similar to the code it is commenting).
Since the dataset is not available, we used the snippets labeled by our team (only the training set) for training the skip-gram model. The idea is to alternate NL tokens and code tokens (see paper for further details).
We trained word2vec using the corpus created.

For doing that, you can run:
```
python3 main.py --train_skipgram
```

The file will be saved as `skipgram_model.model`

## STEP 2: creating the features
In this step we created code features, comment features, and code-comment features (following what explained in the paper).
For code features, we classified each statement based on the categories defined in the paper (+ an OTHER category). We decided to add this extra category for keeping into account all the statements (it is not totally clear in the paper if they ignored all the other statements types for computing the metrics and we decided so)
A statement must contain more than one line. For example
```
for (int i=0; i<10;i++){
System.out.println("I just got executed!");
}
```
is an entire statement (for statement).
With that classification, we computed all the features defined in the paper.
For comment features, we applied the steps defined to the comment (tokenization, stemming, stop word removal) and then we computed the features defined by the authors
For code-comment features, we loaded the skipgram model trained in step 1 and we computed all the similarities written in the paper
Once done this, we merged all results from all features in a single result file (one for each set)

The files like {}_features.csv (with {} in ["train", "eval", "test"]) contain the set of all features from the three categories, while the files like {}_results.csv (with {} in ["train", "eval", "test"]) contain for each set the ID of the row and the result (1 if the comment refers to that block, 0 otherwise).
It may happen that a comment refers only to a part of the statement (e.g., comment may refer only to the body of the for previously defined). In this case we consider that the comment is NOT referring to the for statement, but it is referring to each statement in the body.

For doing that, you can run:
```
python3 main.py --write_all_features
```

## STEP 3: training the random forest
In this step we trained the random forest. From the paper, we inferred the configuration used by the authors and we used that. 

You can run:
```
python3 main.py --train_random_forest
```

## STEP 4: creating result file
In this step we created the result file named `result.csv`, with the configuration chosen by the authors.
We reported each method in the test set, adding the special tokens <start> and <end> at the beginning and at the end of the lines that the random forest predicted to be commented.
You can run:
```
python3 main.py --save_results
```

## STEP4bis: adjusting result file
The model was thought to comment a single block of code (consecutive lines) but we're using it to comment also different blocks (all the cases where the model is predicting 1)
For this reason, we decided to only predict as commented the first block found by the model.
So we can read the `result.csv` file and only keep the first block predicted to be commented
You can run:
```
python3 main.py --adjust_results
```
It will create the file `result_adjusted.csv`.
This file can be used for computing the performance of the baseline.

### STORED FILES

In `input` you can find the final dataset (i.e., the instances that has been considered during the training of the baseline).
In `processed_dataset` you can find the files containing the XML processed with SrcML
In `output/features` you can find the features (used for training the random forest) and the results.
Each row in the features contain a statement of a specific instance with all code, comment and code features. Each row in the result contain for each statement if it is commented or not
In `output/raw_predictions` you can find the raw output of the random forest (i.e., for each statement if it predicts that statement to be referred to the comment or not) and the final result for the comparison with the baseline (with the <start> and <end> tag used for defining the commented block of code)
In `output/results` you can find the results.csv and results_adjusted.csv
In `xml` you can find the xml dataset generated with srcML
`skipgram_model.model` is the skipgram model that we trained



### PACKAGES:
nltk\==3.7
gensim\==4.2.0
scikit-learn\==1.0.1(IMPORTANT, the last version is broken)
javalang\==0.13.0
pandas\==1.4.2
lxml\==4.8.0