{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGdy4uJq1kSQ"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_HNJ04EH1qdW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install -q t5\n",
        "!pip install tensorflow-text==2.12.0\n",
        "\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g65Qu6MF1x2W",
        "outputId": "7fc0adec-775b-4060-b6ec-7562f9385323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies...\n"
          ]
        }
      ],
      "source": [
        "print(\"Installing dependencies...\")\n",
        "import functools\n",
        "import os\n",
        "import gin\n",
        "import tensorflow_gcs_config\n",
        "from google.colab import auth\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "import t5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2h1MRzBLtex2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06f87911-d0fa-440a-b242-529657848522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up GCS access...\n",
            "WARNING: auth.authenticate_user() will eventually stop supporting auth for Tensorflow on TPU devices. See auth.authenticate_service_account() instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU: grpc://10.51.40.210:8470\n"
          ]
        }
      ],
      "source": [
        "TOKENIZER_DIR = \"gs://snippet-summarization/tokenizer\" #@param { type: \"string\" }\n",
        "if not TOKENIZER_DIR or TOKENIZER_DIR == \"gs://\":\n",
        "  raise ValueError(\"You must enter a TOKENIZER_DIR.\")\n",
        "\n",
        "print(\"Setting up GCS access...\")\n",
        "os.environ['USE_AUTH_EPHEM'] = '0'\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set credentials for GCS reading/writing from Colab and TPU.\n",
        "TPU_TOPOLOGY = \"2x2\"\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  TPU_ADDRESS = tpu.get_master()\n",
        "  print('Running on TPU:', TPU_ADDRESS)\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "\n",
        "#LOGGING\n",
        "tf.get_logger().propagate = False\n",
        "py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0kxnD0T8EzI"
      },
      "source": [
        "# Load Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5h4PkSo_Kus",
        "outputId": "71f8064b-54fd-4b74-cf3b-a8ca0536e6d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://snippet-summarization/tokenizer/sp.model\n",
            "gs://snippet-summarization/tokenizer/sp.vocab\n"
          ]
        }
      ],
      "source": [
        "VOCAB_PREFIX = 'sp' #@param {type: \"string\"}\n",
        "vocab_model_path = os.path.join(TOKENIZER_DIR, f'{VOCAB_PREFIX}.model')\n",
        "vocab_path = os.path.join(TOKENIZER_DIR, f'{VOCAB_PREFIX}.vocab')\n",
        "print(vocab_model_path)\n",
        "print(vocab_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-wq4YkN9_UkO"
      },
      "outputs": [],
      "source": [
        "from t5.data import postprocessors as t5_postprocessors\n",
        "from t5.seqio import Feature,SentencePieceVocabulary\n",
        "\n",
        "num_special_mask_tokens = 100 #@param {type: \"integer\"}\n",
        "\n",
        "def load_vocabulary():\n",
        "  return SentencePieceVocabulary(vocab_model_path, num_special_mask_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3yMW-___hYd"
      },
      "source": [
        "# Prepare Dataset for T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "glLJUm1dxIiH"
      },
      "outputs": [],
      "source": [
        "train_path = 'gs://snippet-summarization/data/snippet-summarization/train.tsv' #@param { type: \"string\" }\n",
        "eval_path = 'gs://snippet-summarization/data/snippet-summarization/eval.tsv' #@param { type: \"string\" }\n",
        "test_path = 'gs://snippet-summarization/data/snippet-summarization/test.tsv' #@param { type: \"string\" }\n",
        "\n",
        "finetune_datasets_paths = {\n",
        "    \"train\":      train_path,\n",
        "    \"validation\": eval_path,\n",
        "    \"test\": test_path\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "K0NTLbyXvkCs"
      },
      "outputs": [],
      "source": [
        "def load_dataset(split, shuffle_files=True):\n",
        "  \"\"\"\n",
        "  Function to load .tsv dataset as a tf.data.Dataset in TensorFlow\n",
        "  \"\"\"\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "\n",
        "  ds = tf.data.TextLineDataset(finetune_datasets_paths[split])\n",
        "  ds = ds.map(functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=True)\n",
        "                          ,\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J4_UFsVmSPk"
      },
      "source": [
        "### A few examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__RNWuimAxS9",
        "outputId": "647a9d70-d898-494b-c621-f33e44a034ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'@ Override <nl> protected void paintText ( @ NotNull Graphics graphics , <nl> @ NotNull AbstractButton component , <nl> @ NotNull Rectangle textRect , <nl> @ NotNull String text ) { <nl> CommonDropDownButton button = ( CommonDropDownButton ) component ; <nl> <start> if ( button . getAction ( ) . getShowExpandArrow ( ) ) { <end> <nl>  <nl> <start> textRect . x -= ARROW_REGION_WIDTH / 2 ; <end> <nl> <start> } <end> <nl> super . paintText ( graphics , button , textRect , text ) ; <nl> }<nl>', 'output': b'offset the text rect to reserve space for the arrow'}\n",
            "{'input': b'private void configureListener ( XStream xStream ) <nl> { <nl> xStream . alias ( \"listener\" , Listener . class ) ; <nl> for ( String panelAttribute : LISTENER_ATTRIBUTE ) <nl> { <nl> xStream . aliasAttribute ( Listener . class , panelAttribute , panelAttribute ) ; <nl> } <nl>  <nl> <start> xStream . addImplicitCollection ( Listener . class , \"os\" , OsModel . class ) ; <end> <nl> <start> for ( String osAttribute : OS_ATTRIBUTE ) <end> <nl> <start> { <end> <nl> <start> xStream . aliasAttribute ( OsModel . class , osAttribute , osAttribute ) ; <end> <nl> <start> } <end> <nl> }<nl>', 'output': b'implicit collection for os list in listener'}\n",
            "{'input': b'public static String formatPath ( final String path , final PathQuery pq , final WebConfig config ) { <nl> Path viewPath ; <nl> <start> try { <end> <nl> <start> viewPath = pq . makePath ( path ) ; <end> <nl> <start> } catch ( Throwable t ) { <end> <nl>  <nl> <start> return path ; <end> <nl> <start> } <end> <nl> return formatPath ( viewPath , config ) ; <nl> }<nl>', 'output': b'in all error case return the original string'}\n",
            "{'input': b'public Set < String > selectUnitsByQueryDslAndAccessContract ( MetaDataClient metaDataClient , <nl> SelectMultiQuery select , AccessContractModel accessContractModel ) <nl> throws InvalidParseOperationException , VitamDBException , MetaDataDocumentSizeException , <nl> MetaDataExecutionException , MetaDataClientServerException , InvalidCreateOperationException { <nl>  <nl> <start> select . addUsedProjection ( VitamFieldsHelper . id ( ) ) ; <end> <nl> JsonNode selectWithAccessContractFilter = AccessContractRestrictionHelper <nl> . applyAccessContractRestrictionForUnitForSelect ( select . getFinalSelect ( ) , accessContractModel ) ; <nl> JsonNode resultJson = metaDataClient . selectUnits ( selectWithAccessContractFilter ) ; <nl> Set < String > foundUnitIds = new HashSet < > ( ) ; <nl> for ( JsonNode node : resultJson . get ( RESULTS ) ) { <nl> String id = node . get ( VitamFieldsHelper . id ( ) ) . asText ( ) ; <nl> foundUnitIds . add ( id ) ; <nl> } <nl> return foundUnitIds ; <nl> }<nl>', 'output': b'only return document i d'}\n",
            "{'input': b'@ Override <nl> public void mouseClicked ( MouseEvent event ) <nl> { <nl> <start> if ( event . getSource ( ) == listFramesAvail && ( event . getClickCount ( ) & 1 ) == 0 ) { <end> <nl>  <nl> int idx = listFramesAvail . getSelectedIndex ( ) ; <nl> if ( idx >= 0 ) { <nl> Rectangle rect = listFramesAvail . getCellBounds ( idx , idx ) ; <nl> if ( rect != null && rect . contains ( event . getX ( ) , event . getY ( ) ) ) { <nl> currentCycleAdd ( ) ; <nl> } <nl> } <nl> } else if ( event . getSource ( ) == listCurCycle && ( event . getClickCount ( ) & 1 ) == 0 ) { <nl>  <nl> <start> int idx = listCurCycle . getSelectedIndex ( ) ; <end> <nl> <start> if ( idx >= 0 ) { <end> <nl> <start> Rectangle rect = listCurCycle . getCellBounds ( idx , idx ) ; <end> <nl> <start> if ( rect != null && rect . contains ( event . getX ( ) , event . getY ( ) ) ) { <end> <nl> <start> currentCycleRemove ( ) ; <end> <nl> <start> } <end> <nl> <start> } <end> <nl> <start> } <end> <nl> }<nl>', 'output': b'double click on list element'}\n"
          ]
        }
      ],
      "source": [
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(load_dataset(\"validation\").take(5)):\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsbHi89ZA5-j"
      },
      "source": [
        "# Dataset Prepocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4bJZPQgjxKZ1"
      },
      "outputs": [],
      "source": [
        "from tensorflow_datasets.core.utils.type_utils import Shape\n",
        "def preprocessing(ds):\n",
        "  \"\"\"\n",
        "  Preprocess function to convert the tf.data.Dataset into a text-to-text format,\n",
        "  with both inputs and targets fields.\n",
        "  Param: tf.data.Dataset\n",
        "  Return: text-to-text format\n",
        "  \"\"\"\n",
        "  prefix = 'SNIPPET SUMMARIZATION: ' #@param {type : \"string\"}\n",
        "  def to_inputs_and_targets(ex):\n",
        "    x_input = tf.strings.strip(prefix + ex['input'])\n",
        "    y_label = tf.strings.strip(ex['output'])\n",
        "    inputs = tf.strings.join([x_input], separator=' ')\n",
        "    class_label = tf.strings.join([y_label], separator=' ')\n",
        "    return {'inputs': inputs, 'targets': class_label}\n",
        "  return ds.map(to_inputs_and_targets,\n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vymEYLbQCBRY"
      },
      "source": [
        "### A few examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOyas1ZqBVgE",
        "outputId": "e65dbc58-00b2-4e32-c9ba-289ec4da3153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A few preprocessed train examples...\n",
            "{'inputs': b'SNIPPET SUMMARIZATION: private final void writeFinalWovenProxyMethods ( ) { <nl> // add private fields for the Callable<Object> dispatcher <nl> // and InvocationListener. These aren\\'t static because we can have <nl> // multiple instances of the same proxy class. These should not be <nl> // serialized, or used in JPA or any other thing we can think of, <nl> // so we annotate them as necessary <nl> generateField(DISPATCHER_FIELD, Type.getDescriptor(Callable.class)); <nl> generateField(LISTENER_FIELD, Type.getDescriptor(InvocationListener.class)); <nl> // a general methodAdapter field that we will use to with GeneratorAdapters <nl> // to create the methods required to implement WovenProxy <nl> GeneratorAdapter methodAdapter; <nl> // add a method for unwrapping the dispatcher <nl> methodAdapter = getMethodGenerator(PUBLIC_GENERATED_METHOD_ACCESS, new Method( <nl> \"org_apache_aries_proxy_weaving_WovenProxy_unwrap\", DISPATCHER_TYPE, <nl> NO_ARGS)); <nl> // ///////////////////////////////////////////////////// <nl> // Implement the method <nl> // load this to get the field <nl> methodAdapter.loadThis(); <nl> // get the dispatcher field and return <nl> methodAdapter.getField(typeBeingWoven, DISPATCHER_FIELD, DISPATCHER_TYPE); <nl> methodAdapter.returnValue(); <nl> methodAdapter.endMethod(); <nl> // ///////////////////////////////////////////////////// <nl> // add a method for checking if the dispatcher is set <nl> methodAdapter = getMethodGenerator(PUBLIC_GENERATED_METHOD_ACCESS, new Method( <nl> \"org_apache_aries_proxy_weaving_WovenProxy_isProxyInstance\", <nl> Type.BOOLEAN_TYPE, NO_ARGS)); <nl> // ///////////////////////////////////////////////////// <nl> // Implement the method <nl> // load this to get the field <nl> methodAdapter.loadThis(); <nl>  <nl> <start> Label returnTrueLabel = methodAdapter.newLabel(); <end> <nl> // get the dispatcher field for the stack <nl> methodAdapter.getField(typeBeingWoven, DISPATCHER_FIELD, DISPATCHER_TYPE); <nl> // check if the dispatcher was non-null and goto return true if it was <nl> methodAdapter.ifNonNull(returnTrueLabel); <nl> methodAdapter.loadThis(); <nl> // get the listener field for the stack <nl> methodAdapter.getField(typeBeingWoven, LISTENER_FIELD, LISTENER_TYPE); <nl> // check if the listener field was non-null and goto return true if it was <nl> methodAdapter.ifNonNull(returnTrueLabel); <nl> // return false if we haven\\'t jumped anywhere <nl> methodAdapter.push(false); <nl> methodAdapter.returnValue(); <nl> // mark the returnTrueLable <nl> methodAdapter.mark(returnTrueLabel); <nl> methodAdapter.push(true); <nl> methodAdapter.returnValue(); <nl> // end the method <nl> methodAdapter.endMethod(); <nl> // /////////////////////////////////////////////////////// <nl> } <nl> }<nl>', 'targets': b'make a label for return true'}\n",
            "{'inputs': b'SNIPPET SUMMARIZATION: public float getValue ( ) { <nl>  <nl> <start> return ( float ) slider . getValue ( ) / ratio ; <end> <nl> }<nl>', 'targets': b'return convert real minimum value'}\n",
            "{'inputs': b'SNIPPET SUMMARIZATION: protected void initGuiComponents ( ) { <nl> // Create the tree. It will be used by the table renderer to draw the cells <nl> //in the first column <nl> tree = new CustomJTree(); <nl> tree.setModel(treeTableModel); <nl> tree.setEditable(false); <nl> // Install a tableModel representing the visible rows in the tree. <nl> modelAdapter = new TreeTableModelAdapter(treeTableModel); <nl> super.setModel(modelAdapter); <nl>  <nl> <start> tree.setSelectionModel(new DefaultTreeSelectionModel() { <end> <nl> //extend the constructor <nl> <start> { <end> <nl> <start> setSelectionModel(listSelectionModel); <end> <nl> <start> } <end> <nl> <start> }); <end> <nl> <start> setAutoCreateColumnsFromModel(false); <end> <nl> //Install the renderer and editor <nl> getColumnModel().getColumn(0).setCellRenderer(new TreeTableCellRenderer()); <nl> getColumnModel().getColumn(0).setCellEditor(new TreeTableCellEditor()); <nl> setShowGrid(false); <nl> setRowMargin(0); <nl> } <nl> }<nl>', 'targets': b'force the jtable and jtree to share their row selection model'}\n",
            "{'inputs': b'SNIPPET SUMMARIZATION: @ Override <nl> @ SuppressWarnings ( { \"unchecked\" , \"deprecation\" } ) <nl> public Object getValue ( ELContext context , Object base , Object property ) throws ELException <nl> { <nl> BeanManagerImpl beanManager = webBeansContext . getBeanManagerImpl ( ) ; <nl>  <nl>  <nl> <start> if ( base != null || ! beanManager . isInUse ( ) ) <end> <nl> <start> { <end> <nl> <start> return null ; <end> <nl> <start> } <end> <nl> //Name of the bean <nl> String beanName = (String) property; <nl> //Local store, create if not exist <nl> ELContextStore elContextStore = ELContextStore.getInstance(true); <nl> Object contextualInstance = elContextStore.findBeanByName(beanName); <nl> if(contextualInstance != null) <nl> { <nl> context.setPropertyResolved(true); <nl> return contextualInstance; <nl> } <nl> //Get beans <nl> Set<Bean<?>> beans = beanManager.getBeans(beanName); <nl> //Found? <nl> if(beans != null && !beans.isEmpty()) <nl> { <nl> //Managed bean <nl> Bean<?> bean = beanManager.resolve(beans); <nl> if(bean.getScope().equals(Dependent.class)) <nl> { <nl> contextualInstance = getDependentContextualInstance(beanManager, elContextStore, context, bean); <nl> } <nl> else <nl> { <nl> // now we check for NormalScoped beans <nl> contextualInstance = getNormalScopedContextualInstance(beanManager, elContextStore, context, bean, beanName); <nl> } <nl> } <nl> return contextualInstance; <nl> } <nl> }<nl>', 'targets': b'we only check root bean check if the owb actually got use in this application'}\n",
            "{'inputs': b'SNIPPET SUMMARIZATION: private void load ( InputStream in ) throws IOException { <nl> this . properties . load ( in ) ; <nl> String platform = Utils . getCurrentPlatform ( ) ; <nl> Enumeration < ? > e = properties . propertyNames ( ) ; <nl> while ( e . hasMoreElements ( ) ) { <nl> String key = ( String ) e . nextElement ( ) ; <nl> String value = properties . getProperty ( key ) ; <nl> JSONParser parser = new JSONParser ( ) ; <nl> String newValue ; <nl> try { <nl> Object valueObject = parser . parse ( value ) ; <nl>  <nl> <start> if ( valueObject instanceof Map ) { <end> <nl> <start> JSONObject valueMap = ( JSONObject ) valueObject ; <end> <nl> <start> newValue = ( String ) valueMap . get ( platform ) ; <end> <nl> <start> } else { <end> <nl> newValue = value ; <nl> } <nl> } catch ( ParseException pe ) { <nl> //parsedString is not a smartValue/json object. <nl> newValue = value; <nl> } <nl> properties.setProperty(key, newValue); <nl> } <nl> } <nl> }<nl>', 'targets': b'get value for current platform key'}\n"
          ]
        }
      ],
      "source": [
        "print(\"A few preprocessed train examples...\")\n",
        "sample = tfds.as_numpy(preprocessing(load_dataset(\"train\").take(5)))\n",
        "for ex in sample:\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OkQeLeh8Rst"
      },
      "source": [
        "# Creating Task and Mixture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PobLvzL18zzR",
        "outputId": "d56bb870-fd20-4bbf-fc22-bb45a8c62440"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<seqio.dataset_providers.Mixture at 0x7d0eeac1a590>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "DEFAULT_OUTPUT_FEATURES = {\n",
        "    \"inputs\": Feature(\n",
        "        vocabulary=load_vocabulary(), add_eos=True, required=False),\n",
        "    \"targets\": Feature(\n",
        "        vocabulary=load_vocabulary(), add_eos=True)\n",
        "    }\n",
        "\n",
        "TASK_NAME = \"snippet_summarization\" #@param{ type : \"string\"}\n",
        "\n",
        "# TASK\n",
        "t5.data.TaskRegistry.remove(TASK_NAME)\n",
        "t5.data.TaskRegistry.add(\n",
        "    TASK_NAME,\n",
        "    # Function which returns a tf.data.Dataset\n",
        "    dataset_fn=load_dataset,\n",
        "    splits=[\"train\",\"validation\",\"test\"],\n",
        "    # List of functions that preprocess the input tf.data.Dataset\n",
        "    text_preprocessor=[preprocessing],\n",
        "    # Accuracy is used as evaluation metric\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, t5.evaluation.metrics.bleu],\n",
        "    # Not required, helps for mixing and auto-caching\n",
        "    # num_input_examples=num_input_examples,\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES\n",
        ")\n",
        "\n",
        "MIXTURE_NAME = \"task\" #@param{ type : \"string\"}\n",
        "\n",
        "# MIXTURE\n",
        "t5.data.MixtureRegistry.remove(MIXTURE_NAME)\n",
        "t5.data.MixtureRegistry.add(\n",
        "    MIXTURE_NAME,\n",
        "    # List of tasks\n",
        "    [TASK_NAME],\n",
        "    default_rate=1.0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwqLrVdGB6yy"
      },
      "source": [
        "### A few examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQobwjpVCJbu"
      },
      "source": [
        "# Creating Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5wFs8zZ29Htp"
      },
      "outputs": [],
      "source": [
        "scheduler = \"isr\" #@param [\"polynomial\", \"constant\", \"isr\", \"slanted\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "GKKGuWpXCMNV"
      },
      "outputs": [],
      "source": [
        "# Storage paths\n",
        "FINETUNE_MODEL_DIR = f\"gs://snippet-summarization/models/snippet-summarizer/Best-Performing-Model/Pre-trained\"\n",
        "PRETRAIN_MODEL_DIR='gs://snippet-summarization/models/pre-trained'\n",
        "# FLAGS = tf.app.flags.FLAGS\n",
        "# tf.app.flags.DEFINE_string ('f', '', 'kernel')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QmaDaY39DwxR"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "\n",
        "# Learning rate properties\n",
        "starter_learning_rate = 0.01 #@param {type : \"number\"}\n",
        "end_learning_rate = 0.001 #@param {type : \"number\"}\n",
        "decay_steps = 10000 #@param {type : \"integer\"}\n",
        "\n",
        "learning_rate_fn = PolynomialDecay(\n",
        "     starter_learning_rate,\n",
        "     decay_steps,\n",
        "     end_learning_rate,\n",
        "     power=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3Qx699vN302",
        "outputId": "da672db3-4a4a-4601-bffb-b4bfb8c54023"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function truncated_rsqrt at 0x7d0f89f51a20>\n"
          ]
        }
      ],
      "source": [
        "from mesh_tensorflow.transformer.learning_rate_schedules import slanted_triangular, truncated_rsqrt\n",
        "from t5 import models\n",
        "\n",
        "# Learning rate schedule fn\n",
        "if scheduler == 'polynomial':\n",
        "  learning_rate_scheduler = learning_rate_fn\n",
        "elif scheduler == 'isr':\n",
        "  learning_rate_scheduler = truncated_rsqrt\n",
        "elif scheduler == 'slanted':\n",
        "  learning_rate_scheduler = slanted_triangular\n",
        "else:\n",
        "  learning_rate_scheduler = 0.001\n",
        "\n",
        "print(learning_rate_scheduler)\n",
        "\n",
        "# Model properties\n",
        "MODEL_SIZE = \"small\"\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 16, 100),\n",
        "    \"base\": (2, 128, 8),\n",
        "    \"large\": (8, 64, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
        "\n",
        "\n",
        "# Mesh Tensorflow Transformer\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=FINETUNE_MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    tpu_topology=TPU_TOPOLOGY,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size,\n",
        "    sequence_length={\"inputs\": 1500, \"targets\": 256},\n",
        "    # pick the correct scheduler, according to the model you want to train\n",
        "    learning_rate_schedule = learning_rate_scheduler,\n",
        "    save_checkpoints_steps=5000,\n",
        "    keep_checkpoint_max=keep_checkpoint_max,\n",
        "    iterations_per_loop=100,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR7ZLccg9cwS"
      },
      "source": [
        "# Learning Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4qLQ5I9_9hDR"
      },
      "outputs": [],
      "source": [
        "# Upload one of the four gin files according to the selected scheduler.\n",
        "LOCAL_GIN_PATH = \"/content/operative_config.gin\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4tR2WUl-T96"
      },
      "source": [
        "# Finetuning the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oHp5ScE7nf2",
        "outputId": "59ff437d-d943-44cc-92b1-b3ca6266c7b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:system_path_file_exists:gs://snippet-summarization/models/pre-trained/operative_config.gin\n",
            "ERROR:root:Path not found: gs://snippet-summarization/models/pre-trained/operative_config.gin\n",
            "From /usr/local/lib/python3.10/dist-packages/mesh_tensorflow/transformer/utils.py:2043: TPUConfig.__new__ (from tensorflow_estimator.python.estimator.tpu.tpu_config) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/mesh_tensorflow/transformer/utils.py:2059: RunConfig.__init__ (from tensorflow_estimator.python.estimator.tpu.tpu_config) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_config.py:268: RunConfig.__init__ (from tensorflow_estimator.python.estimator.run_config) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/mesh_tensorflow/transformer/utils.py:2096: TPUEstimator.__init__ (from tensorflow_estimator.python.estimator.tpu.tpu_estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:2811: Estimator.__init__ (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/training_util.py:396: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:2371: StepCounterHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/basic_session_run_hooks.py:686: SecondOrStepTimer.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "WARNING:absl:Using an uncached FunctionDataset for training is not recommended since it often results in insufficient shuffling on restarts, resulting in overfitting. It is highly recommended that you cache this task before training with it or use a data source that supports lower-level shuffling (e.g., FileDataSource).\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "From /usr/local/lib/python3.10/dist-packages/mesh_tensorflow/transformer/utils.py:999: CheckpointSaverHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/mesh_tensorflow/transformer/utils.py:1014: TPUEstimatorSpec.__new__ (from tensorflow_estimator.python.estimator.tpu.tpu_estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:3328: LoggingTensorHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:3369: EstimatorSpec.__new__ (from tensorflow_estimator.python.estimator.model_fn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/estimator.py:1414: NanTensorHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:586: SummarySaverHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saver.py:1176: get_checkpoint_mtimes (from tensorflow.python.checkpoint.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:760: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:1455: SessionRunArgs.__new__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:1454: SessionRunContext.__init__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:1474: SessionRunValues.__new__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n"
          ]
        }
      ],
      "source": [
        "from t5 import models\n",
        "\n",
        "TRAIN_STEPS =  500000#@param {type: \"integer\"}\n",
        "with gin.unlock_config():\n",
        "    gin.parse_config_file(LOCAL_GIN_PATH)\n",
        "    # Stat finetuning\n",
        "    model.finetune(mixture_or_task_name=MIXTURE_NAME,\n",
        "               finetune_steps=TRAIN_STEPS,\n",
        "               pretrained_model_dir=PRETRAIN_MODEL_DIR)\n",
        "    # model.train(mixture_or_task_name=MIXTURE_NAME,\n",
        "    #             steps=TRAIN_STEPS)\n",
        "\n",
        "    model.bach_size=32\n",
        "    model.eval(\n",
        "        mixture_or_task_name=MIXTURE_NAME,\n",
        "        checkpoint_steps=-1,\n",
        "        split=\"test\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9yyAx5NFDoo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}